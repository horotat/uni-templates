\begin{abstract}

%\pagenumbering{arabic}

Robots are gaining more attention in the neuroscientific community as means of 
verifying theoretical models of the development of social skills. In particular, 
humanoid robots with similar physical dimensions as a human child offer a 
controlled platform to simulate interactions between adults and infants. 
Such robots, in interaction with humans and equipped with biologically inspired 
models of social and cognitive skill development, might provide invaluable 
insights into learning mechanisms infants employ when interacting with others.
One such mechanism which develops in infancy is the ability to share and direct 
attention of interacting participants. 
Pointing behaviour underlies joint attention and is preceded by hand-eye 
coordination. 
Here, we attempt to explain how pointing emerges from sensorimotor learning of 
hand-eye coordination in a humanoid robot.
A robot learned joint configurations for different arm postures using random 
body babbling. Arm joint configurations obtained in a babbling experiment were 
used to train internal models which were biologically inspired and based on 
self-organising maps.
We trained and analysed models with various map sizes and depending on their configuration related them to different stages of sensorimotor skill development. 
Finally, we showed that a model based on self-organising maps implemented on a robotic platform accounts for pointing gestures when a human presents an object out of reach for the robot.
\bigskip

\end{abstract}